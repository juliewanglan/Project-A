{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings with BERT (Bidirectional Encoder Representations from Transformers) \n",
    "\n",
    "### What is BERT?\n",
    "BERT is a large state-of-the-art neural network that has been trained on a large corpora of text (millions of sentences). Its applications include but are not limited to:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Question answering systems\n",
    " \n",
    "In this notebook, we walk through how BERT generates fixed-length embeddings (features) from a sentence. You could think of these embeddings as an alternate feature extraction technique compared to bag of words. The BERT model has 2 main components as shown below \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries (if not already installed)\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer (Converting sentences into series of numerical tokens):\n",
    "\n",
    "The tokenizer in BERT is like a translator that converts sentences into a series of numerical tokens that the BERT model can understand. Specifically, it does the following:\n",
    "\n",
    "- Splits Text: It breaks down sentences into smaller pieces called tokens. These tokens can be as short as one character or as long as one word. For example, the word \"chatting\" might be split into \"chat\" and \"##ting\".\n",
    "\n",
    "- Converts Tokens to IDs: Each token has a unique ID in BERT's vocabulary. The tokenizer maps every token to its corresponding ID. This is like looking up the \"meaning\" of the word in BERT's dictionary.\n",
    "\n",
    "- Adds Special Tokens: BERT requires certain special tokens for its tasks, like [CLS] at the beginning of a sentence and [SEP] at the end or between two sentences. The tokenizer adds these in.\n",
    "\n",
    "\n",
    "### Example usage of the tokenizer\n",
    "\n",
    "In the cell below, we see how BERT tokenizes 3 sentences and decodes them back.\n",
    "\n",
    "We'll use the following example sentences:\n",
    "\n",
    "1. \"The sky is blue.\"\n",
    "2. \"Sky is clear today.\"\n",
    "3. \"Look at the clear blue sky.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliewang/micromamba/envs/cs135_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Examples of tokenizing the sentences with BERT\n",
      "----------------------------------------------\n",
      "The sky is blue. is enocoded as : [101, 1109, 3901, 1110, 2221, 119, 102, 0, 0]\n",
      "Sky is clear today. is enocoded as : [101, 5751, 1110, 2330, 2052, 119, 102, 0, 0]\n",
      "Look at the clear blue sky. is enocoded as : [101, 4785, 1120, 1103, 2330, 2221, 3901, 119, 102]\n",
      "----------------------------------------------\n",
      "Examples of decoding the tokens back to English\n",
      "----------------------------------------------\n",
      "Decoded tokens back into text:  [CLS] The sky is blue. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Sky is clear today. [SEP] [PAD] [PAD]\n",
      "Decoded tokens back into text:  [CLS] Look at the clear blue sky. [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliewang/micromamba/envs/cs135_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # Load pre-trained BERT tokenizer and model\n",
    "sentences = [\"The sky is blue.\", \"Sky is clear today.\", \"Look at the clear blue sky.\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(sentences, padding=True,\n",
    "                         max_length=10,\n",
    "                         truncation=True)['input_ids']\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of tokenizing the sentences with BERT')\n",
    "print('----------------------------------------------')\n",
    "for jj, txt in enumerate(sentences):\n",
    "    print('%s is enocoded as : %s'%(txt, encoded_text[jj]))\n",
    "\n",
    "print('----------------------------------------------')\n",
    "print('Examples of decoding the tokens back to English')\n",
    "print('----------------------------------------------')\n",
    "for enc in encoded_text:\n",
    "    decoded_text = tokenizer.decode(enc)\n",
    "    print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model (Extracting meaningful feature representations from the sentences):\n",
    "\n",
    "Once the text is tokenized and converted into the necessary format, it's fed into the BERT model. \n",
    "The **model** processes these inputs to generate contextual embeddings or representations for each token. These representations can then be utilized for various downstream tasks like classification, entity recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bert_embedding\u001b[39m(sentence_list, pooling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     12\u001b[0m     embedding_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1637\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1637\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1625\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1623\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Initialize BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(sentence_list, pooling_strategy='cls'):\n",
    "    embedding_list = []\n",
    "    for nn, sentence in enumerate(sentence_list):\n",
    "        if (nn%100==0)&(nn>0):\n",
    "            print('Done with %d sentences'%nn)\n",
    "        \n",
    "        # Tokenize the sentence and get the output from BERT\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Take the embeddings from the last hidden state (optionally, one can use pooling techniques for different representations)\n",
    "        # Here, we take the [CLS] token representation as the sentence embedding\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "        \n",
    "        # Pooling strategies\n",
    "        if pooling_strategy == \"cls\":\n",
    "            sentence_embedding = last_hidden_states[0]\n",
    "        elif pooling_strategy == \"mean\":\n",
    "            sentence_embedding = torch.mean(last_hidden_states, dim=0)\n",
    "        elif pooling_strategy == \"max\":\n",
    "            sentence_embedding, _ = torch.max(last_hidden_states, dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling strategy: {pooling_strategy}\")\n",
    "        \n",
    "        embedding_list.append(sentence_embedding)\n",
    "    return torch.stack(embedding_list)\n",
    "\n",
    "sentence = [sentences[0]]\n",
    "embedding = get_bert_embedding(sentence)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print('The sentence \"%s\" has been converted to a feature representation of shape %s'%(sentence[0], embedding.numpy().shape))\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print(embedding.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from BERT for the movie review train and test sets\n",
    "\n",
    "Below, we generate the BERT embeddings for the movie reviews dataset provided to you. The embeddings are stored as numpy files in the data_reviews folder:\n",
    "\n",
    "- x_train_BERT_embeddings.npy : matrix of size 2400 x 768 containing 768 length features for each of the 2400 sentences in the training set\n",
    "- x_test_BERT_embeddings.npy : matrix of size 600 x 768 containing 768 length features for each of the 600 sentences in the test set\n",
    "\n",
    "Please note that these embeddings are **already provided to you in the data_reviews folder**. You can directly use the provided feature embeddings as inputs to your choice of classifier. If you would like to generate these feature representations again, run the code cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "x_train_df = pd.read_csv('../data_reviews/x_train.csv')\n",
    "x_test_df = pd.read_csv('../data_reviews/x_test.csv')\n",
    "\n",
    "tr_text_list = x_train_df['text'].values.tolist()\n",
    "te_text_list = x_test_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating embeddings for train sequences...')\n",
    "tr_embedding = get_bert_embedding(tr_text_list)\n",
    "\n",
    "print('Generating embeddings for test sequences...')\n",
    "te_embedding = get_bert_embedding(te_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_embeddings_ND = tr_embedding.numpy()\n",
    "te_embeddings_ND = te_embedding.numpy()\n",
    "\n",
    "save_dir = os.path.abspath('../data_reviews/')\n",
    "print('Saving the train and test embeddings to %s'%save_dir)\n",
    "\n",
    "np.save(os.path.join(save_dir, 'x_train_BERT_embeddings.npy'), tr_embeddings_ND)\n",
    "np.save(os.path.join(save_dir, 'x_test_BERT_embeddings.npy'), te_embeddings_ND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show similarity between reviews using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_k_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some query sentences\n",
    "sentence_id_list = [5, 20, 70, 85, 92, 12, 521, 100, 712]\n",
    "\n",
    "# use K-nearest neighbors to find the 5 reviews that most closely resemble the query review\n",
    "for sentence_id in sentence_id_list:\n",
    "    query_QF = tr_embeddings_ND[sentence_id][np.newaxis, :]\n",
    "    _, nearest_ids_per_query = calc_k_nearest_neighbors(tr_embeddings_ND, query_QF, K=5)\n",
    "    nearest_ids = nearest_ids_per_query[0]\n",
    "\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    print('Reviews that resemble to the sentence : \\n%s'%tr_text_list[sentence_id])\n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    for ii, idx in enumerate(nearest_ids):\n",
    "        print('%d) %s'%(ii, tr_text_list[idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
